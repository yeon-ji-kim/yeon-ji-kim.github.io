<!DOCTYPE html>
<html lang="en">
  <head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-344L93F7Q0"></script>
    <script> 
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-344L93F7Q0');
    </script>
    
    <title>Grasp &mdash; Yeonji Kim</title>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    
    
    <link rel="preconnect" href="https://fonts.gstatic.com">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@300;400;700;900&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Muli:300,400,700,900" rel="stylesheet">
    <link rel="stylesheet" href="fonts/icomoon/style.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">
    <link rel="stylesheet" href="css/jquery-ui.css">
    <link rel="stylesheet" href="css/owl.carousel.min.css">
    <link rel="stylesheet" href="css/owl.theme.default.min.css">
    <link rel="stylesheet" href="css/owl.theme.default.min.css">

    <link rel="stylesheet" href="css/jquery.fancybox.min.css">

    <link rel="stylesheet" href="css/bootstrap-datepicker.css">

    <link rel="stylesheet" href="fonts/flaticon/font/flaticon.css">

    <link rel="stylesheet" href="css/aos.css">

    <link rel="stylesheet" href="css/style.css">

    <link href="./pointer.css" rel="stylesheet">

    <style>
      html {
        scroll-behavior: smooth;
      }
    </style>
    <link rel="icon" href="images/yj.png">
  </head>
  <body data-spy="scroll" data-target=".site-navbar-target" data-offset="300">
  
  <div class="site-wrap">

    <div class="site-mobile-menu site-navbar-target">
      <div class="site-mobile-menu-header">
        <div class="site-mobile-menu-close mt-3">
          <span class="icon-close2 js-menu-toggle"></span>
        </div>
      </div>
      <div class="site-mobile-menu-body"></div>
    </div>
   
    
    <header class="site-navbar py-4 js-sticky-header site-navbar-target" role="banner">
      
      <div class="container-fluid">
        <div class="d-flex align-items-center">
          <div class="site-logo"><a href="index.html"><span style="font-family:'Roboto Slab';">yj</span></a></div>
          <div class="ml-auto">
            <nav class="site-navigation position-relative text-right" role="navigation">
              <ul class="site-menu main-menu js-clone-nav mr-auto d-none d-lg-block">
                <li><a href="index.html" class="nav-link">Work</a></li>
                <li><a href="about.html" class="nav-link">About</a></li>
                <li><a href="YeonjiKim_CV.pdf" target="_blank" class="nav-link">Resume</a></li>
              </ul>
            </nav>
            <a href="#" class="d-inline-block d-lg-none site-menu-toggle js-menu-toggle text-black float-right"><span class="icon-menu h3"></span></a>
          </div>
        </div>
      </div>
      
    </header>


    <!-- highlight Ìö®Í≥º
    <svg xmlns="//www.w3.org/2000/svg" version="1.1" class="svg-filters" style="display:none;">
      <defs>
          <filter id="marker-shape">
              <feTurbulence type="fractalNoise" baseFrequency="0 0.15" numOctaves="1" result="warp" />
              <feDisplacementMap xChannelSelector="R" yChannelSelector="G" scale="30" in="SourceGraphic" in2="warp" />
          </filter>
      </defs>
    </svg>
    -->

    <nav class="floating-menu">
      <a href="#grasp-overview" class="floating-nav">Overview</a><br>
      <a href="#grasp-user-study" class="floating-nav">User Study</a><br>
      <a href="#grasp-tool" class="floating-nav">Tool</a><br>
      <a href="#grasp-findings" class="floating-nav">Findings</a><br>
      <a href="#grasp-reflections" class="floating-nav">Reflections</a>
    </nav>

    <div class="floating-top">
      <a href="#home-section" class="btn-top">ü°π top</a>
    </div>
    
    <div class="intro-section" id="home-section">
      <div class="container">
        <div class="row align-items-center">
          <div class="col-lg-5 mr-auto" data-aos="fade-up">
            <h1>Grasp</h1>
            <p class="mb-3">Understanding Interactive and Explainable Feedback for Supporting Non-Experts with Data Preparation for Building a Deep Learning Model</p>
            <span class="section-sub-title d-block mb-5">#HCI Research &nbsp; #Deep Learning</span>
            <p><a href="https://doi.org/10.7236/IJASC.2020.9.2.90" target="_blank" class="btn btn-primary py-3 px-5">View Full Paper</a></p>
            <!-- github: https://github.com/kyungyeon-lee/Grasp_Final -->

          </div>
          <div class="col-lg-7 mr-auto"  data-aos="fade-up" data-aos-delay="100">
            <figure class="img-absolute-box">
              <img src="images/grasp/grasp_thumbnail.png" alt="Image" class="img-fluid">
            </figure>
          </div>
        </div>
      </div>
    </div>

    <div class="site-section">
      <div class="container">
        <div class="row">
          <div class="col-lg-4 mr-auto mb-4">
            <div class="">
              <p><span class="bold">Type</span><br>Capstone Project (Research Track)</p>
            </div>
            <div class="">
              <p><span class="bold">Duration</span></b><br>2019.02 - 2019.10</p>
            </div>
            <div class="">
              <p><span class="bold">Team</span><br>Yeonji Kim, Kyeongyeon Lee</p>
            </div>
            <div class="">
              <p><span class="bold">Tools</span><br>Adobe XD<br>Excel, Python</p>
            </div>
          </div>
          <div class="col-lg-8">
            <p><span class="bold">My Role</span></b><br>I was responsible for <span class="highlight">user research and design</span> of a tool used in the study, as well as managing the overall process.<br><br>
            üìÇ <span style="font-weight:500;">Literature Review</span> 50%<br>
            üñçÔ∏è <span style="font-weight:500;">UI Design</span> 100%<br>
            üìã <span style="font-weight:500;">User Study</span> (Design Methodology, Conduct User Study) 80%<br>
            üìä <span style="font-weight:500;">Data Analysis</span> (Qualitatively & Quantitatively) 60%<br>
            üìÑ <span style="font-weight:500;">Thesis</span> 70%</p>
            
            <div class="">
              <p><span class="bold">Award</span><br>3rd Place in Ewha Capstone Design Expo</p>
            </div>
          </div>
        </div>
      </div>
    </div>

    <div class="site-section bg-light" id="grasp-overview">
      <div class="container" data-aos="fade-up" data-aos-delay="100">
        <div class="row">
          <div class="col-lg-12 mb-5">
            <h2 class="section-title">Overview</h2>
            <p>It is difficult for non-experts to build machine learning (ML) models at the level that satisfies their needs. 
                Deep learning models are even more challenging because it is unclear how to improve the model, and a trial-and-error approach is not feasible since training these models are time-consuming. 
                To assist these novice users, we examined how <span class="highlight">interactive and explainable feedback</span> while training a deep learning network can contribute to model performance and users‚Äô satisfaction, focusing on the <span class="highlight">data preparation process</span>.<br><br>
                We conducted a user study with <span class="bold">31 participants</span> without expertise, where they were asked to improve the accuracy of a deep learning model, varying feedback conditions. 
                While no significant performance gain was observed, we identified potential barriers during the process and found that interactive and explainable feedback <span class="highlight">provide complementary benefits for improving users‚Äô understanding of ML</span>. 
                We conclude with implications for designing an interface for building ML models for novice users.
              </p>
          </div>
        </div>
      </div>
    </div>

    <div class="site-section" id="#">
      <div class="container" data-aos="fade-up" data-aos-delay="100">
        <div class="row">
          <div class="col-lg-12 mb-5">
            <h2 class="section-title">Challenges</h2>
            <p>
            Studies have shown that it is not straightforward for nonexperts to build ML models without prior knowledge and that they cannot build ML models in an efficient way. 
            For instance, Yang et al. conducted an interview study to understand how non-experts build ML solutions for themselves in real life and revealed several pitfalls. 
            The major problem was that non-experts rarely tried to understand the internal mechanism of learning algorithms, hence had trouble improving the performance or even gave up their tasks. 
            Also, most of them did not consider the overfitting problem, ending up having poor accuracy on a new dataset.</p>
            <p class="bold" style="font-size:1.1rem"><span class="highlight">"How can we better assist non-experts with building a deep learning model?"</span></p>
                <!--
            <br><br><span style="font-size:0.8rem; color:#7d7d7d;">T. Kulesza, M. Burnett, W.-K. Wong, and S. Stumpf, ‚ÄúPrinciples of explanatory debugging to personalize interactive machine learning,‚Äù in Proceedings of the 20th international conference on intelligent user interfaces. ACM, pp. 126‚Äì137, 2015. DOI: https://doi.org/10.1145/2678025.2701399<br>
                T. Kulesza, S. Stumpf, M. Burnett, and I. Kwan, ‚ÄúTell me more?: the effects of mental model soundness on personalizing an intelligent agent,‚Äù in Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. ACM, pp. 1‚Äì10, 2012. DOI: https://doi.org/10.1145/2207676.2207678<br>
                W. B. Knox and P. Stone, ‚ÄúReinforcement learning from human reward: Discounting in episodic tasks,‚Äù in 2012 IEEE RO-MAN: The 21st IEEE International Symposium on Robot and Human Interactive Communication. IEEE, pp. 878‚Äì885, 2012. DOI: https://doi.org/10.1109/roman.2012.6343862<br>
                Q. Yang, J. Suh, N.-C. Chen, and G. Ramos, ‚ÄúGrounding interactive machine learning tool design in how non-experts actually build models,‚Äù in Proceedings of the 2018 Designing Interactive Systems Conference. ACM, pp. 573‚Äì584, 2018. DOI: https://doi.org/10.1145/3196709.3196729</span>
            </p>-->
          </div>
        </div>
      </div>
    </div>

    <div class="site-section bg-light" id="#">
      <div class="container" data-aos="fade-up" data-aos-delay="100">
        <div class="row">
          <div class="col-lg-12 mb-5">
            <h2 class="section-title">Our Approach</h2>
            <p class="mb-3">
              üè∑Ô∏è <span class="bold">Interactivity</span> <br>
              <span style="color:#7d7d7d;">Allowing users to quickly examine the impact of their actions and adapt subsequent inputs to obtain desired behavior when training ML models.</span><br></p>
            <p>
              üè∑Ô∏è <span class="bold">Explainability</span><br>
              <span style="color:#7d7d7d;">Uncovering the process of training or outcomes of an ML system with explanations presented in forms that are easy to understand for human.</span>
            </p>
          </div>
        </div>
      </div>
    </div>

    <div class="site-section" id="process">
      <div class="container" data-aos="fade-up" data-aos-delay="100">
        <div class="row">
          <div class="col-lg-5">
            <div class="mb-5">
              <h2 class="section-title">Process</h2>
            </div>
          </div>
        </div>
        <div class="row mb-5">
          <div class="col-lg-1"></div>
          <div class="col-lg-10">
            <div class="image-absolute-box" >
              <img src="images/grasp/grasp_process.png" alt="Image" class="img-fluid">
            </div>
          </div>
        </div>
      </div>
    </div>

    <div class="site-section bg-light" id="grasp-user-study">
      <div class="container" data-aos="fade-up" data-aos-delay="100">
        <div class="row">
          <div class="col-lg-12">
            <h2 class="section-title" style="color:#9D2721;">User Study</h2>
            <p>To investigate the effectiveness of interactivity and explainability and to identify potential barriers that non-experts would experience while building their ML systems, we conducted a study with 31 participants who had <span class="bold">little or no ML-related experience</span> where they were asked to use our <span class="bold">custom user interface designed for this study to train an image-based ML model.</span>
            </p>
          </div>
        </div>
      </div>
    </div>

    <div class="site-section bg-light" >
      <div class="container" data-aos="fade-up" data-aos-delay="100">
        <div class="row">
          <div class="col-lg-12">
            <h2 class="section-title">Experimental Condition</h2>
          </div>
        </div>
        <div class="row">
          <div class="col-lg-6">
            <p>
              The user study was conducted under the following <span class="highlight">four conditions</span> to examine if different feedback of a model-building tool have <span class="bold">different effects on the model accuracy as well as users‚Äô understanding of ML.</span>
            </p>
          </div>
          <div class="col-lg-6">
            <div class="image-absolute-box" >
              <img src="images/grasp/grasp_condition.png" alt="Image" class="img-fluid">
            </div>
          </div>
        </div>
      </div>
    </div>

    <div class="site-section bg-light" id="#">
      <div class="container" data-aos="fade-up" data-aos-delay="100">
        <div class="row">
          <div class="col-lg-5">
            <div class="mb-5">
              <h2 class="section-title">Participants</h2>
            </div>
          </div>
        </div>
        <div class="row">
          <div class="col-lg-12 mb-3">
            <p>We recruited participants through a local online community, university board, and word of mouth. 
              Thirty-four participants applied, but two were excluded as they reported had studied AI or ML before or have worked on related projects (6 or 7 in Table 1) as our target participants were people who are non-experts in ML. 
              The <span class="bold">rest 32 participants were distributed between 1 and 5</span> (see Table 1) and reported 2.3 on average. <br>
              Table 2 shows the demographics of 31 participants after removing one outlier, where the accuracy performance of the model that the participant built was beyond 3 standard deviations of the mean, throughout the study. 
              Their age range was between 18 to 56, and each participant was randomly assigned to one of the four conditions.
            </p>
          </div>
          <div class="col-lg-6 mb-3">
            <div class="image-absolute-box" >
              <img src="images/grasp/grasp_table1.png" alt="Image" class="img-fluid">
            </div>
          </div>
          <div class="col-lg-6 mb-3">
            <div class="image-absolute-box" >
              <img src="images/grasp/grasp_table2.png" alt="Image" class="img-fluid">
            </div>
          </div>
        </div>
      </div>
    </div>

    <div class="site-section bg-light" id="grasp-tool">
      <div class="container">
        <div class="row" data-aos="fade-up" data-aos-delay="100">
          <div class="col-lg-12 mb-5">
            <h2 class="section-title">Apparatus</h2>
            <p>
              To observe how non-experts create ML systems, we <span class="bold">implemented a web-based system, Grasp,</span> that allows users to <span class="highlight">iteratively import, review, refine the training dataset, and evaluate</span> their trained models to build a deep learning network for image classification.
            </p>
          </div>
        </div>
        <div class="row mb-5" data-aos="fade-up" data-aos-delay="100">
          <div class="col-lg-4">
            <span class="section-sub-title d-block" style="font-size:1.0rem;">Initial Tool Design</span>
          </div>
          <div class="col-lg-8">
            <div class="image-absolute-box" >
              <img src="images/grasp/grasp_initial_design.png" alt="Image" class="img-fluid">
            </div>
          </div>
        </div>
        <div class="row mb-5" data-aos="fade-up" data-aos-delay="100">
          <div class="col-lg-12">
            <span class="section-sub-title d-block" style="font-size:1.0rem;">Actual Tool Design</span>
          </div>
          <div class="col-lg-6">
            <p>
              <span class="bold" style="font-size:1.1rem;">Interactive Feedback</span><br>
              ‚úîÔ∏è <span class="bold">Red-bordered box</span>: misclassified images to users by having a bounding box with red borderlines (1b1)<br>
              ‚úîÔ∏è <span class="bold">Interactive box</span>: classification results with confidences of the current ML model instantly for a selected image (1c1) <br>
              <br>
              <span class="bold" style="font-size:1.1rem;">Explainable Feedback</span><br>
              ‚úîÔ∏è <span class="bold">Activation map</span>: visualizes the areas of input that are important for predictions (1c2), shows how an image is evaluated in the pre-trained ImageNet model<br>
              ‚úîÔ∏è <span class="bold">Confidence table</span>: Top 5 recognition results with the highest confidences based on the same model (1c3) to convey the estimated performance of their model<br>
              ‚úîÔ∏è <span class="bold">Model description</span>: provided after users train their model ‚Äî specifies the number of misclassified images in each class.
            </p>
          </div>
          <div class="col-lg-6">
            <div class="image-absolute-box" >
              <img src="images/grasp/grasp_actual_design.png" alt="Image" class="img-fluid">
            </div>
            <p style="color:#7d7d7d; font-size:0.8rem; text-align: right;"><i>Full condition with interactive and explainable feedback</i></p>
          </div>
        </div>
        <div class="row mb-5" data-aos="fade-up" data-aos-delay="100">
          <div class="col-lg-12">
            <span class="section-sub-title d-block" style="font-size:1.0rem;">System Overview</span>
          </div>
          <div class="col-lg-7">
            <p>
              Our apparatus consists of <span class="bold">two ML models</span>; the primary is <span class="highlight">MobileNetV2</span> that is retrainable and additionally <span class="highlight">VGG16</span>. <br>
              We used MobileNetV2 as the primary model and applied <span class="bold">transfer learning</span> to create image classifiers and generate feedback as quickly as possible by training only a part of the pre-built ML model with smaller number of classes.
              We also had another VGG16 model to visualize the activation map and show the confidence table since generating the activation map is time-consuming (50-60 seconds per image).
            </p>
          </div>
          <div class="col-lg-5">
            <div class="image-absolute-box" >
              <img src="images/grasp/figure1.png" alt="Image" class="img-fluid">
            </div>
          </div>
        </div>
      </div>
    </div>

    <div class="site-section bg-light" >
      <div class="container" data-aos="fade-up" data-aos-delay="100">
        <div class="row">
          <div class="col-lg-12">
            <h2 class="section-title">Procedure</h2>
          </div>
        </div>
        <div class="row mb-5">
          <div class="col-lg-6">
            <p>
              The study was conducted as a one-hour single-session study consisting of a <span class="bold">pretest</span>, a <span class="bold">data preparation task</span> for building a deep learning model, and a <span class="bold">posttest</span> followed by an interview. As a between-subject test, participants were assigned to one of the four conditions for the data preparation task.
            </p>
          </div>
          <div class="col-lg-1"></div>
          <div class="col-lg-5">
            <div class="image-absolute-box" >
              <img src="images/grasp/grasp_procedure.png" alt="Image" class="img-fluid">
            </div>
          </div>
        </div>
        <div class="row">
          <div class="col-lg-12">
            <span class="section-sub-title d-block" style="font-size:1.0rem;">Pretest</span>
            <p class="mb-5">Before performing the data preparation task for building a model, we presented <span class="bold">two ML application scenarios</span> (i.e., image classification, speech recognition), and asked participants to <span class="highlight">explain what kind of dataset would be needed to make such ML systems with high accuracy for each.</span></p>
            <span class="section-sub-title d-block" style="font-size:1.0rem;">Data Preparation Task</span>
            <p class="mb-5">
              For the main task, we first played video tutorials that explain the basic concepts of ML as well as the instructions on how to use our custom interface for performing the task. 
              They were given 3 minutes to explore the interface themselves, such as importing and deleting images and training a model. <br>
              After this brief introduction, participants were instructed to <span class="highlight">add or remove images per class</span> using the web interface <span class="highlight">to improve the accuracy of the initial model as high as possible</span> within 30 minutes. 
              They were allowed to train and check the current model‚Äôs accuracy up to <span class="bold">three times</span>, which can be done by clicking the "Training" button on the top right of the interface. 
              Once the training is completed, participants were able to check the results such as <span class="highlight">training accuracy and validation accuracy</span> on a new page. To understand participants‚Äô mental model, we used a <span class="bold">think-aloud protocol</span> throughout the task and <span class="bold">collected participants‚Äô feedback before and after checking the training results</span> by asking <span class="bold">how they prepared their data and what they think about the results</span>.
            </p>
            <span class="section-sub-title d-block" style="font-size:1.0rem;">Posttest & Interview</span>
            <p>
              After the task completion, we had a posttest session where we asked participants the <span class="bold">same questions</span> again with the two scenarios from our pretest to examine if <span class="highlight">participants‚Äô understanding of ML systems has improved after performing our task</span>. 
              Then we had a wrap-up interview to <span class="highlight">understand the potential barriers</span> for preparing a dataset for an ML model and how the presented interactive or explainable feedback can be used to ease the training process. 
              We also asked participants about their satisfaction, usefulness, the difficulty of each feature they used as well as perceived task load (NASA-TLX) on a 7-point scale.
            </p>
        
          </div>
        </div>
      </div>
    </div>

    <div class="site-section bg-light" id="#">
      <div class="container" data-aos="fade-up" data-aos-delay="100">
        <div class="row">
          <div class="col-lg-12">
            <div class="mb-5">
              <h2 class="section-title">Data & Analysis</h2>
            </div>
          </div>
        </div>
        <div class="row">
          <div class="col-lg-6">
            <div class="image-absolute-box" >
              <img src="images/grasp/data.png" alt="Image" class="img-fluid">
            </div>
          </div>
          <div class="col-lg-6">
            <div class="image-absolute-box" >
              <img src="images/grasp/analysis.png" alt="Image" class="img-fluid">
            </div>
          </div>
        </div>
      </div>
    </div>

    <div class="site-section" id="grasp-findings">
      <div class="container">
        <div class="row mb-5" data-aos="fade-up" data-aos-delay="100">
          <div class="col-lg-12 mr-auto">
            <h2 class="section-title" style="color:#9D2721;">Findings</h2>
            <p>
              Here we present the potential effects of interactive and explainable feedback in terms of accuracy, the understanding of ML, and subjective responses collected from participants for each feedback feature.
            </p>
          </div>
        </div>
        <div class="row mb-5" data-aos="fade-up" data-aos-delay="100">
          <div class="col-lg-12 mr-auto">
            <div class="process p-3">
              <span class="number">01</span>
              <div>
                <h2 class="section-title">Understanding of Novices</h2>
                <p class="bold" style="font-size: 1.1rem;">The Understanding of Building a Better ML Model</p>
              </div>
            </div>
          </div>
          <div class="col-lg-12">
            <p>To examine if participants‚Äô understanding of constructing ML models has been improved after performing the task, one researcher graded participants‚Äô pretest and posttest responses where the participants‚Äô group was blinded. 
              For each scenario per test, responses of participants were given a score between 0 and 4 depending on how much they are aware of the fact that <span class="highlight">increasing volume</span> and <span class="highlight">variety of training data</span> can help to improve the accuracy (2 scores each per metric). 
            </p>
          </div>
          <div class="col-lg-7">
            <p>
            As a result, we found that <span class="bold">participants‚Äô understanding of how data should be prepared to improve the accuracy of an ML model has been improved for all conditions</span>. 
            Participants in <span class="bold">full condition showed the biggest improvements</span> on average, followed by explainable, default, and interactive conditions. 
            <br><br>
            Interestingly, while the performed task is about image classification, the scores suggest that building an image classification model can also <span class="highlight">help to deepen the understanding of the speech recognition model</span>.
            </p>
          </div>
          <div class="col-lg-5">
            <div class="image-absolute-box" >
              <img src="images/grasp/finding1.png" alt="Image" class="img-fluid">
            </div>
          </div>
        </div>

        <div class="row mb-5" data-aos="fade-up" data-aos-delay="100">
          <div class="col-lg-12 mr-auto">
            <div class="process p-3">
              <span class="number">02</span>
              <div>
                <h2 class="section-title">Feedback Features</h2>
                <p class="bold" style="font-size: 1.1rem;">The Use and Subjective Assessments of Feedback Features</p>
              </div>
            </div>
          </div>
          <div class="col-lg-12">
            <p>We further assessed how each feature for each feedback type assisted participants in preparing a dataset for building ML models by analyzing participants‚Äô subjective feedback such as contribution (i.e., how much this feature has contributed to their decision when preparing the data) and usefulness at the end of the study. 
            </p>
          </div>
          <div class="col-lg-5">
            <div class="image-absolute-box" >
              <img src="images/grasp/IN_features.png" alt="Image" class="img-fluid">
            </div>
          </div>
          <div class="col-lg-7">
            <div class="image-absolute-box" >
              <img src="images/grasp/EP_features.png" alt="Image" class="img-fluid">
            </div>
          </div>
        </div>

        <div class="row" data-aos="fade-up" data-aos-delay="100">
          <div class="col-lg-12 mr-auto">
            <div class="process p-3">
              <span class="number">03</span>
              <div>
                <h2 class="section-title">Observed Behaviors</h2>
                <p class="bold" style="font-size: 1.1rem;">Perceived Task Loads and Observed Behaviors of Novices</p>
              </div>
            </div>
          </div>
          <div class="col-lg-12">
            <span class="section-sub-title d-block" style="font-size:1.0rem;">Perceived Task Load</span>
          </div>
          <div class="col-lg-7">
            <p>
              When compared to the responses collected from experts, we have confirmed that mental demand is slightly higher for participants. On the other hand, other responses were similar, including effort. This suggests that novice users‚Äô perceived task load is not too different from experts except for mental demand.
            </p>
          </div>
          <div class="col-lg-4 mb-3">
            <div class="image-absolute-box" >
              <img src="images/grasp/NASA-TLX.png" alt="Image" class="img-fluid">
            </div>
          </div>
          <div class="col-lg-12 mb-5">
            <span class="section-sub-title d-block" style="font-size:1.0rem;">Amount of Data Novices Prepared</span>
            <p>
              <span class="highlight">The number of images that participants used for training varied.</span> More than half of the participants started with 20-40 images per class and maintained the number throughout the task (N = 17) while the other 6 participants gradually increase or decrease the number of training data. This is contrary to the behavior of experts who started training the model with a large volume of data. In the study, only the remaining eight added images as many as possible from the start, assuming that adding many images would increase the accuracy, especially for F8, who related the performance with the word ‚Äòbig data.‚Äô
            </p>
          </div>
          <div class="col-lg-12">
            <span class="section-sub-title d-block" style="font-size:1.0rem;">Tendency to Over-clean the Data</span>
            <p>
              To understand users‚Äô mental models, we observed how participants built their ML models while they are performing the task. 
              As a result, we found that participants had a <span class="highlight">tendency to clean the data as much as possible</span> without knowing that this <span class="bold">can cause an overfitting problem</span>. 
              To be specific, participants tended to remove images if dogs with different breeds or people are included for the selected class even if the correct dog is present. 
              This behavior was more <span class="bold">frequently observed</span> from the participants in <span class="bold">default condition</span>, which had the minimum feedback without interactive nor explainable feedback. 
              Moreover, when adding new pictures, most participants added images if and only if the images are <span class="highlight">representative of that class</span>, such as images where the frontal face of a single dog with the correct breed is visible. 
              This indicates that non-expert users believe that having only the representative data can lead to higher accuracy without knowing that their model would not perform well on real-world data with large variations (noises).
            </p>
          </div>
        </div>
      </div>
    </div>

    <div class="site-section bg-light" id="grasp-reflections">
      <div class="container" data-aos="fade-up" data-aos-delay="100">
        <div class="row">
          <div class="col-lg-12">
            <h2 class="section-title">Reflections</h2>
            <span class="section-sub-title d-block" style="font-size:1.0rem; color:#333333;">Working on a Research Project for the First Time</span>
            <p class="mb-5">
              Working on a research project, and even leading a team of two, requires being able to be precise in designing methodology, writing a tool guide for participants, and managing all the data from the study.
              I also realized that there are many kinds of variables that can happen during the experiment. Do not try to predict users! 
              After many iterations of pilot studies, I could see how users utilize the system, which may have been different with our purpose. 
              But that was the most exciting part of the user research!
            </p>
            <span class="section-sub-title d-block" style="font-size:1.0rem; color:#333333;">What's Essential for MVP?</span>
            <p>
              Since we had time limitation for implementing the tool, we had difficulties in building interpretable deep learning systems, which were complicated and time-consuming.
              As an alternative, we took confidence tables as our explainable features.
              Even though the outcome turned different from what we have planned, I learned that making time-efficient choices and compromising is necessary for teams to move on.
            </p>
          </div>
        </div>
      </div>
    </div>

    <div class="site-section bg-light" id="">
      <div class="container" data-aos="fade-up" data-aos-delay="100">
        <div class="row">
          <div class="col-lg-2 mr-auto" style="font-size: 0.9rem; font-weight:500;">
            <a href="" style="color:#333333;">ü°† See previous project</a>
          </div>
          <div class="col-lg-8 mr-auto">
          </div>
          <div class="col-lg-2 mr-auto" style="font-size: 0.9rem; font-weight:500;">
              <a href="" style="color:#333333;">See next project ü°¢</a>
          </div>
        </div>
      </div>
    </div>
     
    <footer class="footer-section">
      <div class="container">
        <div class="row text-center">
          <div class="col-md-12">
            <h3>Get in Touch</h3>
            <a href="mailto:yeonjikim@ewhain.net" style="color:gray;"><img src="images/email-icon.png" width="40"></a>&nbsp
            <a href="https://www.linkedin.com/in/yeonji-kim/" target="_blank" style="color:gray;"><img src="images/linkedin-icon.png" width="40"></a>
          </div>
        </div>
      </div>
    </footer>

  
    
  </div> <!-- .site-wrap -->

  <script src="js/jquery-3.3.1.min.js"></script>
  <script src="js/jquery-migrate-3.0.1.min.js"></script>
  <script src="js/jquery-ui.js"></script>
  <script src="js/popper.min.js"></script>
  <script src="js/bootstrap.min.js"></script>
  <script src="js/owl.carousel.min.js"></script>
  <script src="js/jquery.stellar.min.js"></script>
  <script src="js/jquery.countdown.min.js"></script>
  <script src="js/bootstrap-datepicker.min.js"></script>
  <script src="js/jquery.easing.1.3.js"></script>
  <script src="js/aos.js"></script>
  <script src="js/jquery.fancybox.min.js"></script>
  <script src="js/jquery.sticky.js"></script>

  
  <script src="js/main.js"></script>
    
  </body>
  <script src="./pointer.js"></script>
  <script>
    init_pointer();
    init_pointer({
      pointerColor: "#9d2721"
    })
    init_pointer({
      pointerColor: "#9d2721",
      ringSize: 20,
      ringClickSize: 10
    })
  </script>
</html>